**Welcome to the Lir Tutorial**

To use this tutorial, open the source file `anagrams.lir`
(available in the [repository of the
tutorial](https://github.com/borisvassilev/lir-tutorial)) in a text
editor and start reading.  Every time you see a "`----`" on a line by
itself, it is a good time to stop reading for a moment and try to
compile the source file up to that point.  This can be done by simply
invoking `lir` with the file as its only argument:

~~~~
$ lir anagrams.lir
~~~~

If everything goes well, a final document, `anagrams.html`, will be
generated in your working directory.  Open it in your web browser!

The file up to here is just a plain text file, formatted using the
"markdown" markup recognized by Pandoc.  The whole first line is surrounded
by double asterisk (`**`) for strong emphasis.  In the first paragraph,
there is inline code surrounded by single backticks (`` ` ``).  The
second paragraph is a single-line block of source code verbatim, with a line
"`~~~~`" on bottom and on top; usually, this is not necessary in a Lir
source file.

Now let's try to generate the first version of the final document:

- You can copy everything up to (and including) the 4-dash line below to
  file named `anagrams.lir` in your working directory.
- You can also use the link below to download this file.
- For `git` users: you can use the version of `anagrams.lir` in this
  repository [here](https://github.com/borisvassilev/lir-tutorial/blob/master/docs/1/anagrams.lir).

Now, run Lir.  Notice how the list above was correctly recognized and
typeset differently.

Get 
[source](https://github.com/borisvassilev/lir-tutorial/blob/master/docs/1/anagrams.lir)
and [final document](https://borisvassilev.github.io/lir-tutorial/1/anagrams.html)


----

# Document formatting and structure
The previous part contains a few examples of using markdown to format your
text.  In this part, we will see more uses of markdown.  At the beginning
of this paragraph, there is a line starting with a hash sign (`#`).  This
defines a section heading: in the final document, "Document formatting and
structure" will be typeset differently, and it will appear in a table of
contents at the beginning of the document.

First, let's give our data analysis an aim and a name.  We want to find
anagrams of words in the English language.  We will call the data analysis
"Anagrams", and give our name as an author.

---
title: 'Lir Tutorial: Finding Anagrams'
author: Boris Vassilev
...

(This will not appear in the final document at that spot: it will be used
to create the document title.)

## Sections and subsections
By using 1, 2, or 3 hash signs at the beginning of the line, we define
sections, subsections, subsubsections in the final document.

## Links
Here is a [link to the Pandoc User's Guide](http://pandoc.org/MANUAL.html).
I usually need to keep this guide open while I'm writing a Lir source file.

# Making the final document
In order to be able to use Lir efficiently, you need to learn at least a
bit about the program Make and the syntax of Makefiles.  Here, we will
define a Makefile for our data analysis.  It tells make
that to make all files, it needs to generate the file `anagrams.html` by
running the program `lir` with `anagrams.lir` as its first and only
argument. 
<<Makefile>>=
name = anagrams

all: $(name).html

$(name).html: $(name).lir
	lir $(name).lir

@
This is our first code chunk. Its name is "Makefile"; its contents begin on
the line after its declaration and run up to the `@` on a
line by itself. The contents of this code chunk can go to the file named
`Makefile` in the working directory of the project. To do this, we could
use the following invocation of `notangle`, part of the Noweb distribution:

~~~~
$ notangle -t8 -RMakefile anagrams.lir > Makefile
~~~~

> Run `notangle`; leave tabs untouched; extract the code chunk named
> "Makefile" from the source file `anagrams.lir` to the file `Makefile`.

Usually, we don't have to do this on our own: Lir recognizes code chunks
that should be extracted to files, and does that automatically.

If you look at the final document, you will see that the
[[<<Makefile>>]] code chunk is numbered "C1" (Code chunk 1), and right
under the header it reads, "root chunk".  A root chunk is a code chunk
that is not referenced by any other code chunks.  In other words, there
are no code chunks in this document that are using the code chunk
[[<<Makefile>>]].

The double brackets define inline code in `noweb`.  These are especially
useful when you refer to a code chunk by name, as the references to
[[<<Makefile>>]] above: in the final document, these are going to be
links to the definition of that code chunk.

After saving the contents of [[<<Makefile>>]] above to a file `Makefile` in
the working directory, you can simply invoke `make` (without any arguments)
to run Lir.

Get 
[source](https://github.com/borisvassilev/lir-tutorial/blob/master/docs/2/anagrams.lir)
and [final document](https://borisvassilev.github.io/lir-tutorial/2/anagrams.html)


----

# Aim
The aim of this tutorial is to show you how to use Lir efficiently and
effectively.  To achieve this, the tutorial describes the solution to a
simple problem: how to find anagrams in the English language.

The idea is to follow the solution outlined by Bentley in "Programming
Pearls" [@bentley1986programming].  We are going to prepare a dictionary of
words, calculate the signature of each word, sort by signatures.  At that
point, all sets of words that are anagrams of each other will be in
consecutive runs, signed by the same signature.  We will squash them, each
set (with more than two words in it) displayed on a space-separated line.

In the previous paragraph, we have cited the 1986 book "Programming
Pearls".  In the working directory, there is a bibliography file in the
format recognized by Biblatex (I use [JabRef](https://www.jabref.org/)
to maintain that file, but any reference manager can export to a `.bib`
file format).  As long as there is a bibliography file with the same
name as the Lir source file and the `.bib` extension, Lir will generate
the citations and insert a list of citations at the end of the final
document.  Because this source file is `anagrams.lir`, the bibliography
file was saved as `anagrams.bib`.

From now on, the document will end with the beginning of a last section,
"Bibliography", so that it is listed in the Table of Contents.

Get 
[source](https://github.com/borisvassilev/lir-tutorial/blob/master/docs/3/anagrams.lir)
and [final document](https://borisvassilev.github.io/lir-tutorial/3/anagrams.html)


----

# Select words
What is a dictionary that is useful as input to our program?  What exactly
is the program going to do?  We want to find a signature for every word in
the dictionary.  To do that, we will sort the characters of each word.  If
a two different words have the same signature, this means they have the
exact same letters, so, they are anagrams of each other.

I will use [the dictionary available with the Linux
distribution](/usr/share/dict/words).  This file was copied to the working
directory under the name `en-dict`, and declared as an input file (source).
<<:source en-dict>>=
@
The last two lines are the first example of a special code chunk that
declares a source (input data) for our work flow.  The name of the code
chunk must begin with the string ":source", followed by a single space
character, and a path to the file.  The path may be an absolute path, but
it is much easier to use a path relative to the working directory.

This dictionary is meant to be used by other programs, for example to check
spelling.  It contains duplicate version of many words, only with the first
letter capitalized or suffixed by an "'s".  To get rid of these duplicates,
we define a bash script.  It contains three components in a pipe:

<<clean-en-dict.sh>>=
<<Delete all words with apostrophes>> \
        | <<Set all letters to lower case>> \
        | <<Sort, leaving only unique entries>>
@
The code chunk [[<<clean-en-dict.sh>>]] is used to define the pipe that
cleans the dictionary file. It is a root chunk: it is not referenced by
name in any other code chunk. It contains references to other code chunks
by name.

The first chunk referenced above is
[[<<Delete all words with apostrophes>>]].
To to this, we use `sed`:
<<Delete all words with apostrophes>>=
sed -n "/'/!p"
@

Then comes [[<<Set all letters to lower case>>]].
It is a call to a SWI-Prolog script that converts all letters to lower
case.
<<Set all letters to lower case>>=
<<Run SWI-Prolog script>> to-lower-case.prolog
@
The script [[<<to-lower-case.prolog>>]] has to be declared as a dependency
of the Bash script that uses it:
<<:make>>=
clean-en-dict.sh: to-lower-case.prolog

@
The SWI-Prolog script reads all input, [converts it to lower
case](http://eu.swi-prolog.org/pldoc/doc_for?object=string_lower/2), and
writes it to output.  I decided to use SWI-Prolog for this because it
can deal with Unicode, and because you should have it on your machine if
Lir is installed.
<<to-lower-case.prolog>>=
main :-
        read_string(current_input, _, Input),
        string_lower(Input, Output),
        format(current_output, "~s", [Output]).
@
To safely run this and any Prolog program that defines a `main`, we can do:
<<Run SWI-Prolog script>>=
swipl -q -O -g 'main,halt' -t 'halt(1)'
@
While it is not strictly necessary to define this in a code chunk of its
own, this code chunk might prove useful later, if we have to run another
SWI-Prolog script.

At the end, everything is sorted, leaving only unique entries.
<<Sort, leaving only unique entries>>=
sort --unique
@

We define the transformation from the original file `en-dict` to the
cleaned `en-words` as another rule:
<<:make>>=
en-words: en-dict clean-en-dict.sh
	bash clean-en-dict.sh < $< > $@

@
There is an empty line after the second line: it is necessary in order to
finish the rule.

The first line of this rule reads:

> To generate the file `en-words`, you need the files `en-dict` and
> `clean-en-dict.sh`.

The second line of the rule reads:

> Run the bash script `clean-en-dict.sh` to convert the first prerequisite
> `en-dict` (written as `$<`) to create the target `en-words` (written as
> `$@`).

Note the use of the two [Make automatic
variables](https://www.gnu.org/software/make/manual/html_node/Automatic-Variables.html#Automatic-Variables):
`$<` stands for "the first prerequisite, and `$@` stands for "the target".
It is not strictly necessary to use automatic variables in the rules we
define with Lir.  Instead, the rule could have just as well been written
with the full file names:
<<: Another way to declare the rule>>=
en-words: en-dict clean-en-dict.sh
	bash clean-en-dict.sh < en-dict > en-words

@
However, it is good practice to use the automatic variables: it is easier
to change and reuse the rules (more on that later).

The name of the above code chunk, [[<<: Another way to declare the rule>>]],
starts with a colon (`:`), but there is no keyword like `make` or `source`
following it.  This code chunk will appear in the final document, but it will
not be considered an executable program by Lir.  I tend to use such code
chunks to show code verbatim in my final document.

Finally, to have a vague idea if this all worked, we can add a display item
with the sizes of the original and the cleaned dictionary.  Immediately
below the placeholder for the listing is the rule for generating the result
using `wc`.
<<:listing en-sizes>>=
title: Word file sizes.
<<:make>>=
en-sizes: en-words en-dict
	wc --lines $^ > $@

@
In this last ":make" code chunk, I have used the `$^` Make special
variable, "all prerequisites to a target".

Get 
[source](https://github.com/borisvassilev/lir-tutorial/blob/master/docs/4/anagrams.lir)
and [final document](https://borisvassilev.github.io/lir-tutorial/4/anagrams.html)


----

# Recap
Now is a good moment to stop and summarize everything we have done so far.

- Defined a valid Lir source file;
- Formatted and structured the file using markdown;
- Defined a code chunk and tangled it to a file;
- Defined a Bash program and an SWI-Prolog program;
- Declared rules for file dependencies and for executing programs;
- Inserted a listing of a result file to a display item in the final document.
- Along the way, generated the final document for every version of the Lir
  source file.

## Relax
At this point, you might be feeling a bit overwhelmed by Lir.  Why do we
need to use code chunks?  Why do we need to use Make?  Why do we need to
generate display items?

There are two answers for these question.  The first goes: It is so easy
to do all these things with Lir, that it doesn't add anything to your
workload.

To give the second answer, we need some background.  Consider for a
moment how you would generate the results from a series of programs?  At
first, you define your programs.  Then, as they add up, you start using
a script to run the programs for you.  At some point you realize you are
generating again and again intermediate files that cannot have changed.
You start using Make to deal with the dependencies, so you end up
replacing your run script with a makefile.  And if you happen to need to
share your work with someone, you have to provide some documentation to
go with your code: code that is now scattered among all the source files
and the makefile.

Here is where Lir comes in.  You keep the source of all your executable
files in the Lir source file.  You can even document it a bit, as long
as it doesn't slow you down too much.  From the start, you declare the
dependencies between your input, your code, and your results.
Extracting the source files, running them, displaying the results: all
that is done automatically by Lir.

Get 
[source](https://github.com/borisvassilev/lir-tutorial/blob/master/docs/5/anagrams.lir)
and [final document](https://borisvassilev.github.io/lir-tutorial/5/anagrams.html)


----

# How it's made
By now you should have some understanding of _what_ Lir does.  Now, we
will go into some detail on _how_ Lir does it.

If you have used Lir to interpret your Lir source file, there will be a
hidden directory `.lir` inside your working directory.  Go ahead and
delete `.lir` (you can use the command `rm -r .lir` to do that).  Don't
be afraid: nothing of value will be lost.

When we invoke Lir using `lir anagrams.lir` or `make` using the
[[<<Makefile>>]] defined above, Lir performs three steps in simple
succession:

1. "Tangle" the Lir source file;
2. "Make" everything;
3. "Weave" the final document.

The three steps are implemented as calls to the `lir` command line
program with the command `tangle`, `make`, or `weave` as the first
argument and the Lir source file name as the second argument.

## Tangle
Now, run the first step only:

<<: Tangle the source file>>=
$ lir tangle anagrams.lir
@

... and look inside the `.lir` folder. This is what I see:

<<: Contents of the .lir folder after tangling>>=
$ ls -a .lir
.                 __LIR_SOURCE_anagrams.lir  .makehtml
..                .makeall                   to-lower-case.prolog
clean-en-dict.sh  .makedag
en-dict           Makefile
@

Here is how these files came to be, and why they are there (note the use
of a `#.` for autonumbered bullet points):

#.  Before anything else, Lir creates the `.lir` directory if it does not
    yet exist.

#.  Lir looks for executable programs in your Lir source file.  All root code
    chunks that are not special code chunks (name does not begin with a
    colon) are extracted to files inside `.lir`.  The file names are the code
    chunk names: for now, we see `clean-dict-en-dict.sh` and
    `to-lower-case.prolog` listed there.  You can open these in your text
    editor: go ahead and do it!

#.  Lir generates three makefiles in `.lir`: `.makeall`, `.makedag`, and
    `.makehtml`.  Look at them in your text editor!

    The file `.makeall` contains a rule that lists all
    input files declared in the Lir source file as prerequisites for
    making the target `all`.

    The file `.makedag` is a concatenation of all the [[<<:make>>]] code chunks
    defined in the Lir source file.

    The file `.makehtml` contains the rule for generating the final
    document.

#.  Lir generates a normalized version of the Lir source file.  The name
    of this normalized version is prefixed by `__LIR_SOURCE_`.  If you
    are really ambitious, go ahead and use `diff` to see how the
    normalized version is different from the original Lir source.

At this point, Lir has created a "state" for the work flow: it has
tangled all executable code so that it can be evaluated, and generated
makefiles so that code can be evaluated while observing the dependencies
in the work flow.

## Make
To evaluate the state, run the second step (don't forget to change back
to the working directory):

<<: Make all results>>=
$ lir make anagrams.lir
@

You should see that the two programs are evaluated: first,
`clean-dict-sh` cleans `en-dict` to obtain `en-words`; then, `wc` is
used to show the size of both `en-dict` and `en-words`, and saves the
output to `en-sizes`.

You can look for the files `en-words` and `en-sizes` inside `.lir`.  At
the moment, those are all the results generated by the work flow.

## Weave
To generate the final document, run the last step:
<<: Weave the final document>>=
$ lir weave anagrams.lir
@

The final HTML document is generated and copied from `.lir` to the
working directory.  You can open it in your web browser (as you have
been doing all along).

There is a practical advantage of running the three steps -- Tangle,
Make, and Weave -- separately.  The second step, Make, can be run on a
different machine: a server with enough memory to handle real-word data
sets, for example.  To do that, one would only have to copy the state
(the `.lir` directory) to the remote server, run `make` there, and then
copy back the state, with all generated results, to the local machine
for Weaving.

Get 
[source](https://github.com/borisvassilev/lir-tutorial/blob/master/docs/6/anagrams.lir)
and [final document](https://borisvassilev.github.io/lir-tutorial/6/anagrams.html)


----

# Anagrams
By now we have covered all the basics of using Lir.  Finally, let's
apply Lir to solve the problem of finding anagrams in the English
language, as promised.  From here on, we move a bit faster, as there is
less to explain.

We will implement the idea presented earlier: see [Aim](#aim) (this is a
cross-reference to a section heading, as described in the [Pandoc
manual](http://pandoc.org/MANUAL.html#header-identifiers).  Here is a
diagram of the complete data flow of our implementation (data objects
are written in small letters and DATA TRANSFORMATIONS are in all caps):
<<: Data flow for finding anagrams>>=
         +--< words >--+
         |             |
         |             |
       SIGN            |
         |             |
         v             |
    signatures         |
         v             |
         |             |
         +--> PASTE <--+
                v
                v
       SORT BY SIGNATURES
                v
                v
    +---< sorted-signed >---+
    |                       |
    |                       |
    v                       v
 COLUMN 1                COLUMN 2
    v                       v
    v                       |
COUNT RUNS                  |
    v                       |
    v           words-sorted-by-signature
 set-sizes                  |
    v                       |
    +-------> SQUASH <------+
                v
                v
           anagram-sets
@
The word "squash" means "writing consecutive lines with the same
signature to a space-separated list of anagrams on a line".  This use of
the word comes from the discussion of this problem in "Programming
Pearls" [@bentley1986programming].

To make this into a work flow that can be evaluated, we start by
declaring the dependencies and rules:
<<:make>>=
en-signatures: sign.prolog en-words
	<<Run SWI-Prolog script>> $^ > $@

en-sorted-signed: paste-and-sort.sh en-signatures en-words
	bash $^ > $@

@
Note that here, we have put `PASTE` and `SORT BY SIGNATURES` from the
diagram above in the same step, because there is no reason not to.

Similarly, we put `COLUMN 1` and `COUNT RUNS` in the same rule.
<<:make>>=
en-set-sizes: set-sizes.sh en-sorted-signed
	bash $^ > $@

en-words-sorted-by-signature: en-sorted-signed
	cut --field=2 $^ > $@

@
Usually, avoid evaluating standard command line tools directly in the
rules.  The reason is that if the rules change, Lir will not notice,
because there is no executable code (code in non-special root chunks)
that has changed.  In this isolated case, it is fine to leave it like
this.

<<:make>>=
en-anagram-sets: squash en-set-sizes en-words-sorted-by-signature
	./$^ > $@

@
This is it!

This leaves us with a question: why didn't we generate the rules from
the diagram, or the diagram from the rules?  Both are definitely doable,
and probably not a bad idea.  However, this would mean still more
programs that would have to be involved in this work flow.  It is not in
the scope of this tutorial, but a good exercise for the reader.

## Generate signatures
We need signatures such that words that are anagrams of each other have
the same signature.  The approach we use here has been independently
discovered by many programmers: if you sort the letters of two anagrams,
you get the same result.

To generate the signatures, we use Prolog again.  This program opens for
reading the file passed as its first command line argument, and writes
all signatures to standard output:
<<sign.prolog>>=
main :-
        current_prolog_flag(argv, [Words_file|_]),
        setup_call_cleanup(open(Words_file, read, In),
                output_signatures(In, current_output),
                close(In)).
@

To output signatures, we [read from the input stream line by
line](http://eu.swi-prolog.org/pldoc/doc_for?object=read_line_to_codes/2)
(because each word is on a line of its own), sort the letters of the
word [without removing
duplicates](http://eu.swi-prolog.org/pldoc/doc_for?object=msort/2), and
write the sorted letters to the output stream.
<<sign.prolog>>=
output_signatures(In, Out) :-
        read_line_to_codes(In, Word),
        (       Word == end_of_file
        ->      true
        ;       msort(Word, Signature),
                format(Out, "~s\n", [Signature]),
                output_signatures(In, Out)
        ).
@
This last code chunk is an example of a code chunk continuation.  If a
code chunk with the same name already appeared earlier in the Lir source
file, the contents will be appended to it.  Interrupting a code chunk
like this is the Lir equivalent of a comment block within a source code
file.

## Sort by signatures
This will be a Bash script that uses the standard command line tools
`paste` and `sort`.  `Paste` will take two files with the same number of
lines, and merge them in two columns separated by a tab.  We use `sort`
with two options: `--key=1` says that it should sort by the value in the
first column only, and `--stable` says that it sould ignore the rest of
the line.  Because the second column is already sorted, adding the
option `--stable` has no effect on the final result; however, it makes
our intentions clear, and happens to do a bit less work (it even runs
slightly faster).
<<paste-and-sort.sh>>=
paste "$1" "$2" \
        | sort --key=1 --stable
@

## Anagram set sizes
First, we have to generate the size of each set of anagrams.  Because
the file `en-sorted-signed` is sorted by signatures (its first column),
it is enough to extract the first column and count the number of
occurences of the same line.  For this, we use `cut` and `uniq --count`.

For reasons shouded in mistery, the output of `uniq --count` is a bit
unconventional and cannot be configured.  Instead of using columns
separated by a tab (or any other delimiter), it outputs the counts in a
right-justified, space-padded column, separated from the next column
with a single space.  We will use Awk to get the numbers only.
<<set-sizes.sh>>=
cut --fields=1 "$1" \
        | uniq --count \
        | awk '{ print $1 }'
@

## Squash
Finally, we need a program that takes uses the set sizes and the words
to squash sets of anagrams to lines.  All programs presented so far have
been interpreted, without the additional step of compiling a source code
file to an executable program.  For the sake of the example, the final
program, `squash`, is implemented in C++.

It takes two arguments: the file with the counts, and the list of words.
<<squash.cpp>>=
<<Includes>>
<<Helper functions>>
int main(int argc, char* argv[])
{
        if (argc < 3) return 1;
        std::ifstream counts{argv[1]};
        if (!counts) return 2;
        std::ifstream words{argv[2]};
        if (!words) return 3;

        <<Squash words using counts>>
}
@
You should notice that here, we define the source code file
`squash.cpp`, but in the rules declared earlier, we used the compiled
program `squash`.  This works because Make knows how to [compile most
common programming
languages](https://www.gnu.org/software/make/manual/html_node/Implicit-Rules.html#Implicit-Rules).
An added bonus is that we don't say which C++ compiler should be used to
compile `squash.cpp`, and let Make choose it.  This adds a little bit of
portability to the work flow defined in this Lir source file.

We need however to add a flag for C++ compilation, to make sure that a
relatively recent C++ standard is used:
<<:make>>=
CXXFLAGS += -std=c++0x
@

To squas words, we read one set size at a time from `counts`.  If the
set size is greater than 1, we squash that number of lines to a single
line.  Otherwise, it is exactly 1, and we ignore a single line from
`words`.
<<Squash words using counts>>=
size_t n;
while (counts >> n)
        if (n > 1) {<<Squash `n` words>>}
        else {<<Ignore one line>>}
@

To squash `n` words, we put the first word on a line, then add a space
and another words, until we run out of words in this set of anagrams; at
that point, we end the line.
<<Squash `n` words>>=
copy_line(words, std::cout);
for (size_t i = 1; words && i < n; ++i) {
        std::cout.put(' ');
        copy_line(words, std::cout);
}
std::cout.put('\n');
@

To ignore a single line, we use
[`std::basic_istream::ignore()`](http://en.cppreference.com/w/cpp/io/basic_istream/ignore),
with a new line as the delimiter.
<<Ignore one line>>=
words.ignore(std::numeric_limits<std::streamsize>::max(),
             '\n');
@

We need a single helper function that copies one line from an input
stream to an output stream, dropping the new line.
<<Helper functions>>=
<<Definition of `copy_line()`>>
@

This is a template function not only because we want to be fancy
Committing to a type, especially with the C++ Standard Library types, is
both error-prone and requires a lot of typing.  Instead, we let the
compiler decide if the call to `copy_line()` works for this definition.
<<Definition of `copy_line()`>>=
template< <<Input stream type>> I,
          <<Output stream type>> O >
void copy_line(I& in, O& out)
{
        in.get(*out.rdbuf());
        in.ignore();
}
@

This is an example of using literate programming to address a perceived
short-comming in the programming language.  While discussed for many
years, there is still no official support for "concepts" in all C++
compilers.  In other words, it is possible to tell the compiler that the
template types need to conform to a certain specification.
These two definitions document our intentions:
<<Input stream type>>=
typename
@
<<Output stream type>>=
typename
@

Finally, we need `iostream` for writing to standard output, `fstream`
for reading from files, and `limits` for the maximum number of
characters that can be skipped at once.
<<Includes>>=
#include <iostream>
#include <fstream>
#include <limits>
@

Now, we can query the plain text database that we have generated in
`en-anagram-sets`.  For example, to see which are the largest sets of
anagrams, using Awk:
<<:make>>=
largest-anagram-sets: find-largest-anagram-sets.awk en-anagram-sets
	awk -f $^ > $@

<<find-largest-anagram-sets.awk>>=
BEGIN { max = 2 }
{       if (NF > max) {
                split("", sets)
        }
        if (NF >= max) {
                sets[$0] = 0
                max = NF
        }
}
END {
        for (s in sets) { print s }
}
@

<<:listing largest-anagram-sets>>=
title: The largest set(s) of anagrams
caption: There is a set of 8 anagrams!
@
Note that I added the caption to this listing only after I generated the
listing and had a chance to look at it.  This is how I prefer to use
Lir:

#. Generate a result;
#. Look at the result;
#. Document the result;
#. Decide what the next step will be;
#. Implement the next step;
#. GOTO 1.

This concludes the tutorial, for now.

Get 
[source](https://github.com/borisvassilev/lir-tutorial/blob/master/docs/7/anagrams.lir)
and [final document](https://borisvassilev.github.io/lir-tutorial/7/anagrams.html)


----

This work is licenced under the terms of the GPL (see [COPYING](https://github.com/borisvassilev/lir-tutorial/blob/master/COPYING)).

Copyright (c) 2016 Boris Vassilev

# Bibliography
